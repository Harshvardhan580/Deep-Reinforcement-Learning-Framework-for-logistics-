{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import deque\n",
    "import ast  # For safely evaluating strings as Python literals\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the synthetic dataset\n",
    "dataset = pd.read_csv(\"C:\\\\Users\\\\hp\\\\Desktop\\\\College\\\\Optimizing Assignment\\\\synthetic_meal_delivery_dataset_with_previous_time.csv\")\n",
    "\n",
    "# Convert state columns from strings to dictionaries\n",
    "dataset[\"Current State\"] = dataset[\"Current State\"].apply(ast.literal_eval)\n",
    "dataset[\"Next State\"] = dataset[\"Next State\"].apply(ast.literal_eval)\n",
    "\n",
    "# Define neural network for Q-values\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Experience Replay with Prioritized Sampling\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.priorities = deque(maxlen=capacity)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def add(self, experience, error):\n",
    "        self.memory.append(experience)\n",
    "        priority = (abs(error) + 1e-5) ** self.alpha\n",
    "        self.priorities.append(priority)\n",
    "\n",
    "    def sample(self, batch_size, beta=0.4):\n",
    "        probabilities = np.array(self.priorities) / sum(self.priorities)\n",
    "        indices = np.random.choice(len(self.memory), batch_size, p=probabilities)\n",
    "        experiences = [self.memory[i] for i in indices]\n",
    "        weights = (len(self.memory) * probabilities[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        return experiences, weights, indices\n",
    "\n",
    "    def update_priorities(self, indices, errors):\n",
    "        for i, error in zip(indices, errors):\n",
    "            self.priorities[i] = (abs(error) + 1e-5) ** self.alpha\n",
    "\n",
    "# Define the RL Agent\n",
    "class DDQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, gamma=0.9, lr=0.001, batch_size=64, memory_size=30000):\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.memory = PrioritizedReplayBuffer(memory_size)\n",
    "        self.main_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network = QNetwork(state_dim, action_dim).to(self.device)\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.main_network.parameters(), lr=lr)\n",
    "        self.epsilon = 1.0  # Initial exploration rate\n",
    "        self.epsilon_min = 0.1\n",
    "        self.epsilon_decay = 0.999\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.action_dim - 1)\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            q_values = self.main_network(state)\n",
    "        return torch.argmax(q_values).item()\n",
    "\n",
    "    def remember(self, experience, error):\n",
    "        self.memory.add(experience, error)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        experiences, weights, indices = self.memory.sample(self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        weights = torch.tensor(weights, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        states = torch.tensor(states, dtype=torch.float32).to(self.device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long).to(self.device)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(self.device)\n",
    "        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(self.device)\n",
    "\n",
    "        # Q-value predictions\n",
    "        q_values = self.main_network(states).gather(1, actions.unsqueeze(-1)).squeeze(-1)\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.target_network(next_states).max(1)[0]\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        # Compute loss\n",
    "        loss = (weights * (q_values - target_q_values) ** 2).mean()\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update priorities\n",
    "        errors = torch.abs(q_values - target_q_values).cpu().detach().numpy()\n",
    "        self.memory.update_priorities(indices, errors)\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "\n",
    "    def save_model(self, path):\n",
    "        torch.save(self.main_network.state_dict(), path)\n",
    "\n",
    "    def load_model(self, path):\n",
    "        self.main_network.load_state_dict(torch.load(path, map_location=self.device))\n",
    "        self.target_network.load_state_dict(self.main_network.state_dict())\n",
    "\n",
    "# Training process\n",
    "def train_agent(agent, dataset, episodes=1000, target_update_freq=200):\n",
    "    for episode in range(1, episodes + 1):\n",
    "        state = dataset.iloc[0][\"Current State\"]  # This is now a dictionary\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            reward = dataset.iloc[0][\"Reward\"]\n",
    "            next_state = dataset.iloc[0][\"Next State\"]  # This is now a dictionary\n",
    "            done = dataset.iloc[0][\"Done\"]\n",
    "\n",
    "            agent.remember((\n",
    "                [state[\"Expected Delivery Time (mins)\"], state[\"Distance to Depot\"], state[\"Distance to Restaurants\"]],\n",
    "                action, reward, \n",
    "                [next_state[\"Expected Delivery Time (mins)\"], next_state[\"Distance to Depot\"], next_state[\"Distance to Restaurants\"]],\n",
    "                done\n",
    "            ), error=reward)\n",
    "            state = next_state\n",
    "\n",
    "            agent.replay()\n",
    "            if episode % target_update_freq == 0:\n",
    "                agent.update_target_network()\n",
    "\n",
    "        if agent.epsilon > agent.epsilon_min:\n",
    "            agent.epsilon *= agent.epsilon_decay\n",
    "\n",
    "    print(\"Training completed.\")\n",
    "\n",
    "# Hyperparameters\n",
    "state_dim = 3  # (Expected Delivery Time, Distance to Depot, Distance to Restaurants)\n",
    "action_dim = 4  # (Accept, Reject, Return to Depot, Move to Restaurant)\n",
    "memory_size = 30000\n",
    "batch_size = 64\n",
    "gamma = 0.9\n",
    "learning_rate = 0.001\n",
    "target_update_freq = 200\n",
    "episodes = 1000\n",
    "\n",
    "# Initialize the agent\n",
    "agent = DDQNAgent(state_dim, action_dim, gamma=gamma, lr=learning_rate, batch_size=batch_size, memory_size=memory_size)\n",
    "\n",
    "# Train the agent\n",
    "train_agent(agent, dataset, episodes=episodes, target_update_freq=target_update_freq)\n",
    "\n",
    "# Save the trained model\n",
    "model_path = \"ddqn_per_meal_delivery_model.pth\"\n",
    "agent.save_model(model_path)\n",
    "print(f\"Model saved at {model_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
